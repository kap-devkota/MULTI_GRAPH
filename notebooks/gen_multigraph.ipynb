{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv \n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_folder = \"../datasets/STRING/networks/\"\n",
    "networks       = [\"coocurrence\", \n",
    "                  \"database\",\n",
    "                  \"fusion\",\n",
    "                  \"neighbor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RWR matrix\n",
    "def compute_rwr(A, restart_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Computing RWR matrix.\n",
    "    \"\"\"\n",
    "    d    = A @ np.ones((A.shape[0], 1))\n",
    "    \n",
    "    # P  = D^-1 A\n",
    "    P    = A / d\n",
    "    \n",
    "    n, _ = P.shape\n",
    "    return pinv(np.identity(n) - restart_prob * P) * (1 - restart_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000440005</td>\n",
       "      <td>0.050</td>\n",
       "      <td>coocurrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000349467</td>\n",
       "      <td>0.332</td>\n",
       "      <td>coocurrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000392147</td>\n",
       "      <td>0.236</td>\n",
       "      <td>coocurrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000221138</td>\n",
       "      <td>0.327</td>\n",
       "      <td>coocurrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000369126</td>\n",
       "      <td>0.209</td>\n",
       "      <td>coocurrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895885</th>\n",
       "      <td>9606.ENSP00000485638</td>\n",
       "      <td>9606.ENSP00000295822</td>\n",
       "      <td>0.053</td>\n",
       "      <td>neighbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895886</th>\n",
       "      <td>9606.ENSP00000485638</td>\n",
       "      <td>9606.ENSP00000364815</td>\n",
       "      <td>0.049</td>\n",
       "      <td>neighbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895887</th>\n",
       "      <td>9606.ENSP00000485638</td>\n",
       "      <td>9606.ENSP00000410186</td>\n",
       "      <td>0.045</td>\n",
       "      <td>neighbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895888</th>\n",
       "      <td>9606.ENSP00000485638</td>\n",
       "      <td>9606.ENSP00000355890</td>\n",
       "      <td>0.198</td>\n",
       "      <td>neighbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895889</th>\n",
       "      <td>9606.ENSP00000485638</td>\n",
       "      <td>9606.ENSP00000322775</td>\n",
       "      <td>0.049</td>\n",
       "      <td>neighbor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>895890 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                     1      2         type\n",
       "0       9606.ENSP00000000233  9606.ENSP00000440005  0.050  coocurrence\n",
       "1       9606.ENSP00000000233  9606.ENSP00000349467  0.332  coocurrence\n",
       "2       9606.ENSP00000000233  9606.ENSP00000392147  0.236  coocurrence\n",
       "3       9606.ENSP00000000233  9606.ENSP00000221138  0.327  coocurrence\n",
       "4       9606.ENSP00000000233  9606.ENSP00000369126  0.209  coocurrence\n",
       "...                      ...                   ...    ...          ...\n",
       "895885  9606.ENSP00000485638  9606.ENSP00000295822  0.053     neighbor\n",
       "895886  9606.ENSP00000485638  9606.ENSP00000364815  0.049     neighbor\n",
       "895887  9606.ENSP00000485638  9606.ENSP00000410186  0.045     neighbor\n",
       "895888  9606.ENSP00000485638  9606.ENSP00000355890  0.198     neighbor\n",
       "895889  9606.ENSP00000485638  9606.ENSP00000322775  0.049     neighbor\n",
       "\n",
       "[895890 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine All networks into one\n",
    "df    = {}\n",
    "nodes = {}\n",
    "nodemaps = {}\n",
    "all_nodes = set()\n",
    "for net in networks:\n",
    "    df[net]         = pd.read_csv(f\"{network_folder}{net}.txt\", sep = \"\\t\", header = None)\n",
    "    df[net][\"type\"] = net\n",
    "    nodes[net]    = list(set(df[net][0]).union(set(df[net][1])))\n",
    "    nodemaps[net] = {k: int(i) for i,k in enumerate(nodes[net])} \n",
    "    all_nodes     = all_nodes.union(set(nodes[net]))\n",
    "\n",
    "all_df = pd.concat([df[net] for net in networks]).reset_index(drop = True)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_in = all_df.drop([2], axis = 1).groupby([0, 1])[\"type\"].apply(list).reset_index(name = \"in\")\\n\\ndf_in.drop([\"nodes_present\"], axis = 1)\\n\\ndef check_if_node_exists(row):\\n    out_row = []\\n    for net in networks:\\n        if row[0] in nodes[net] and row[1] in nodes[net]:\\n            out_row.append(net)\\n    return out_row\\n\\ndf_in[\"nodes_present\"] = df_in.apply(check_if_node_exists, axis = 1)\\ndf_in.columns = [\"p\", \"q\", \"in\", \"nodes_present\"]\\n\\ndf_in\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_in = all_df.drop([2], axis = 1).groupby([0, 1])[\"type\"].apply(list).reset_index(name = \"in\")\n",
    "\n",
    "df_in.drop([\"nodes_present\"], axis = 1)\n",
    "\n",
    "def check_if_node_exists(row):\n",
    "    out_row = []\n",
    "    for net in networks:\n",
    "        if row[0] in nodes[net] and row[1] in nodes[net]:\n",
    "            out_row.append(net)\n",
    "    return out_row\n",
    "\n",
    "df_in[\"nodes_present\"] = df_in.apply(check_if_node_exists, axis = 1)\n",
    "df_in.columns = [\"p\", \"q\", \"in\", \"nodes_present\"]\n",
    "\n",
    "df_in\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(\"../datasets/STRING/networks/coo_fuse_data_neighbor.txt\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Adjacency matrices for each network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the As\n",
    "As = {}\n",
    "for net in networks:\n",
    "    As[net] = np.zeros((len(nodemaps[net]), len(nodemaps[net])))\n",
    "    for p_, q_, w, _ in df[net].values:\n",
    "        p           = nodemaps[net][p_]\n",
    "        q           = nodemaps[net][q_]\n",
    "        As[net][p, q] = w\n",
    "        As[net][q, p] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the RWR matrices for each network\n",
    "\n",
    "<p> The RWR restart probability is chosen to be 0.5.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing RWR of the network coocurrence\n",
      "Computing RWR of the network database\n",
      "Computing RWR of the network fusion\n",
      "Computing RWR of the network neighbor\n"
     ]
    }
   ],
   "source": [
    "# Compute the RWR matrices\n",
    "Ps = {}\n",
    "for net in networks:\n",
    "    print(f\"Computing RWR of the network {net}\")\n",
    "    Ps[net] = compute_rwr(As[net])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Saving...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "OUTNET = \"../datasets/STRING/RWR+ADJ\"\n",
    "for net in networks:\n",
    "    np.save(f\"{OUTNET}/{net}.adj.npy\", As[net])\n",
    "    np.save(f\"{OUTNET}/{net}.rwr.npy\", Ps[net])\n",
    "    with open(f\"{OUTNET}/{net}.json\", \"w\") as oj:\n",
    "        json.dump(nodemaps[net], oj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency matrix final computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final output is the matrix R\n",
    "all_nodeset = {k: i for i, k in enumerate(all_nodes)}\n",
    "\n",
    "R = np.zeros((len(all_nodes), \n",
    "              len(all_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "for p1, q1, in_, node_pres in df_in.values:\n",
    "    score = 0\n",
    "    node_pres = ast.literal_eval(node_pres)    \n",
    "    for net in node_pres:\n",
    "        p     = nodemaps[net][p1]\n",
    "        q     = nodemaps[net][q1]\n",
    "        score += As[net][p, q]\n",
    "        score += (Ps[net][p, q] + Ps[net][q, p]) / 2\n",
    "    score /= len(node_pres)\n",
    "    ap    = all_nodeset[p1]\n",
    "    aq    = all_nodeset[q1]\n",
    "    R[ap, aq] = score\n",
    "    R[aq, ap] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "np.save(\"../datasets/STRING/networks/coo_fuse_data_neighbors.npy\", R)\n",
    "with open(\"../datasets/STRING/networks/coo_fuse_data_neighbors.json\", \"w\") as jf:\n",
    "    json.dump(all_nodeset, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "R = np.load(\"../datasets/STRING/networks/coo_fuse_data_neighbors.npy\")\n",
    "with open(\"../datasets/STRING/networks/coo_fuse_data_neighbors.json\", \"r\") as oj:\n",
    "    R_map = json.load(oj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/glide\")\n",
    "from glide_compute import glide_compute_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "glide_mat, glide_map = glide_compute_map((R, \n",
    "                                          R_map))\n",
    "\n",
    "# Saving\n",
    "np.save(\"../datasets/STRING/networks/coo_fuse_data_neighbors.glide.npy\", glide_mat)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/scoring/\")\n",
    "import scoring\n",
    "import predict \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "gmat = np.load(\"../datasets/STRING/networks/coo_fuse_data_neighbors.glide.npy\")\n",
    "with open(\"../datasets/STRING/networks/coo_fuse_data_neighbors.json\", \"r\") as gf:\n",
    "    gmap = json.load(gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import graph_io\n",
    "import pandas as pd\n",
    "def create_predictor_glidemat(glide_mat, k = 10, is_wt = True, confidence = True, params = {}):\n",
    "    \"\"\"\n",
    "    GLIDE-mat to prediction\n",
    "    \"\"\"\n",
    "    node_associations = {}\n",
    "    n_nodes         = glide_mat.shape[0]\n",
    "    for node in range(n_nodes):\n",
    "        assoc_list = np.argsort(-glide_mat[node])[: k]\n",
    "        assoc_dict = {alist: glide_mat[node, alist] for alist in assoc_list}\n",
    "        node_associations[node] = assoc_dict\n",
    "        # node_association : {dict protein1 -> {dict protein2 -> weight}}. number of protein1 = n_nodes,\n",
    "        # number of protein2 for each protein1 = k.\n",
    "    def predictor(training_labels):\n",
    "        \"\"\"\n",
    "        Use the node_associations to find the appropriate nearest neighbors\n",
    "        \"\"\"\n",
    "        tlabels_f = lambda i: (training_labels[i] if i in training_labels else [])\n",
    "        return predict.glide(node_associations, tlabels_f, confidence = confidence)\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def HGNC_STRING(locname):\n",
    "    \"\"\"\n",
    "    STRING to protein name converter\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(locname, sep = \"\\t\")\n",
    "    string_name = df[\"#string_protein_id\"]\n",
    "    hgnc_name   = df[\"preferred_name\"]\n",
    "    return {key:value for key, value in zip(hgnc_name, string_name)}\n",
    "    \n",
    "\n",
    "def entrez_dict(IS_SYMBOL = True):\n",
    "    if not IS_SYMBOL:\n",
    "        ensp_dict = {}\n",
    "        header    = True\n",
    "        with open(\"/cluster/tufts/cowenlab/Projects/Multiple_Graphs/dataset/9606.protein.info.v11.5.txt\", \"r\") as of:\n",
    "            for line in of:\n",
    "                if header:\n",
    "                    header = False\n",
    "                    continue\n",
    "                words = re.split(\"\\t\", line.strip())\n",
    "                if len(words) >= 2 and not words[1].startswith(\"ENSG\"):\n",
    "                    ensp_dict[words[1]] = words[0]\n",
    "            \n",
    "    s_e_dict = {}\n",
    "    with open(\"/cluster/tufts/cowenlab/Projects/Denoising_Experiments/shared_data/dream_files/idmap.csv\", \"r\") as of:\n",
    "        header = True\n",
    "        for line in of:\n",
    "            if header:\n",
    "                header = False\n",
    "                continue\n",
    "            words = re.split(\"\\t\", line.strip())\n",
    "            if len(words) >= 2 and words[1] != \"\":\n",
    "                if not IS_SYMBOL and words[0] in ensp_dict:\n",
    "                    s_e_dict[ensp_dict[words[0]]] = int(words[1])\n",
    "                else:\n",
    "                    s_e_dict[words[0]] = int(words[1])\n",
    "    rev_dict = {s_e_dict[k]: k for k in s_e_dict}\n",
    "    return s_e_dict, rev_dict\n",
    "\n",
    "def get_labels(go_type, min_level, min_prot, node_list, is_STRING = False):\n",
    "    filter_protein = {\"namespace\": go_type, \"lower_bound\": min_prot}\n",
    "    filter_labels  = {\"namespace\": go_type, \"min_level\": min_level}\n",
    "    filter_parents = {\"namespace\": go_type}\n",
    "    # Using entrez dict to do symbol->entrez mapping\n",
    "    s_entrez, entrez_s = entrez_dict()\n",
    "\n",
    "    if is_STRING:\n",
    "        hgnc_string_map = HGNC_STRING(\"/cluster/tufts/cowenlab/Projects/Denoising_Experiments/9606.protein.info.v11.5.txt\")\n",
    "        s_entrez = {hgnc_string_map[key]: value for key, value in s_entrez.items() if key in hgnc_string_map}\n",
    "        entrez_s = {value:key for key, value in s_entrez.items()}\n",
    "    e_symbols          = [s_entrez[k] for k in node_list if k in s_entrez]\n",
    "\n",
    "    f_labels, labels_dict, parent_dict = graph_io.get_go_labels_and_parents(\"../datasets/GO/go-basic.obo\",\n",
    "                                                                            \"../datasets/GO/gene2go\",\n",
    "                                                                            filter_protein, \n",
    "                                                                            filter_labels,\n",
    "                                                                            filter_parents,\n",
    "                                                                            e_symbols,\n",
    "                                                                            anno_map = lambda x: entrez_s[x])\n",
    "    proteins_to_go     = {}\n",
    "    for l in labels_dict:\n",
    "        prots = labels_dict[l]\n",
    "        for p in prots:\n",
    "            if p not in proteins_to_go:\n",
    "                proteins_to_go[p] = []\n",
    "            proteins_to_go[p] += [l]\n",
    "    labels = {i: proteins_to_go[node_list[i]] \n",
    "              for i in range(len(node_list)) if node_list[i] in proteins_to_go}\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMS:0:00:04.489010 335,350 annotations, 20,702 genes, 18,726 GOs, 1 taxids READ: ../datasets/GO/gene2go \n",
      "18674 IDs in loaded association branch, molecular_function\n",
      "  EXISTS: ../datasets/GO/go-basic.obo\n",
      "../datasets/GO/go-basic.obo: fmt(1.2) rel(2021-12-15) 47,157 GO Terms; optional_attrs(relationship)\n",
      "Read gene2go.dat File\n",
      "Number of Labels: 40\n",
      "HMS:0:00:04.650525 335,350 annotations, 20,702 genes, 18,726 GOs, 1 taxids READ: ../datasets/GO/gene2go \n",
      "18674 IDs in loaded association branch, cellular_component\n",
      "  EXISTS: ../datasets/GO/go-basic.obo\n",
      "../datasets/GO/go-basic.obo: fmt(1.2) rel(2021-12-15) 47,157 GO Terms; optional_attrs(relationship)\n",
      "Read gene2go.dat File\n",
      "Number of Labels: 85\n",
      "HMS:0:00:04.655303 335,350 annotations, 20,702 genes, 18,726 GOs, 1 taxids READ: ../datasets/GO/gene2go \n",
      "18674 IDs in loaded association branch, biological_process\n",
      "  EXISTS: ../datasets/GO/go-basic.obo\n",
      "../datasets/GO/go-basic.obo: fmt(1.2) rel(2021-12-15) 47,157 GO Terms; optional_attrs(relationship)\n",
      "Read gene2go.dat File\n",
      "Number of Labels: 207\n"
     ]
    }
   ],
   "source": [
    "# Computing the GO labels\n",
    "labels = {}\n",
    "gos = [\"molecular_function\", \"cellular_component\", \"biological_process\"]\n",
    "for GO_TYPE in gos:\n",
    "    labels[GO_TYPE] = get_labels(GO_TYPE, 5, 50, list(gmap.keys()), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for go in gos:\n",
    "    kfold = 5\n",
    "    f1 = scoring.kfoldcv_with_pr(kfold, \n",
    "                            labels[go], \n",
    "                            create_predictor_glidemat(gmat, \n",
    "                                                    k = 10, \n",
    "                                                    is_wt = False))\n",
    "    acc = scoring.kfoldcv(kfold,\n",
    "                    labels[go],\n",
    "                    create_predictor_glidemat(gmat, \n",
    "                                            k = 10, \n",
    "                                            is_wt = False, \n",
    "                                            confidence = False)) \n",
    "    results.append((go, f1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('molecular_function',\n",
       "  [0.5971742735941947,\n",
       "   0.5968815563292742,\n",
       "   0.5700622610150747,\n",
       "   0.5919819866669759,\n",
       "   0.5903995856925937],\n",
       "  [0.5498154981549815,\n",
       "   0.6005535055350554,\n",
       "   0.5784132841328413,\n",
       "   0.6033210332103321,\n",
       "   0.5854779411764706]),\n",
       " ('cellular_component',\n",
       "  [0.5482389428955244,\n",
       "   0.5448515450266416,\n",
       "   0.5527816406506847,\n",
       "   0.5608234837731302,\n",
       "   0.5535590414115613],\n",
       "  [0.6093489148580968,\n",
       "   0.6026711185308848,\n",
       "   0.6054535336672231,\n",
       "   0.5915414579855315,\n",
       "   0.6238888888888889]),\n",
       " ('biological_process',\n",
       "  [0.44615718123694054,\n",
       "   0.42890897499174613,\n",
       "   0.4337744686280131,\n",
       "   0.4311428682938999,\n",
       "   0.4252612557830509],\n",
       "  [0.47805171377029465,\n",
       "   0.48947684906794947,\n",
       "   0.48947684906794947,\n",
       "   0.4930847865303668,\n",
       "   0.46490701859628075])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5950184501845018,\n",
       " 0.5830258302583026,\n",
       " 0.5581180811808119,\n",
       " 0.5913284132841329,\n",
       " 0.5818014705882353]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise",
   "language": "python",
   "name": "denoise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
